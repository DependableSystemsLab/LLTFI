{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(\"Python version\")\n",
        "print (sys.version)\n",
        "print(\"Version info.\")\n",
        "print (sys.version_info)"
      ],
      "metadata": {
        "id": "S54rfa079yEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz9ImoqlyCyv"
      },
      "outputs": [],
      "source": [
        "# Required installation\n",
        "!pip install transformers\n",
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "!pip install onnxruntime_extensions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import onnxruntime\n",
        "from transformers import GPT2Model, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from onnx import numpy_helper\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nqa_w0NN22Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT2**\n",
        "\n",
        "There are 3 ONNX models available in the ONNX repo. (https://github.com/onnx/models/tree/main/text/machine_comprehension). **gpt2-lm-head** is chosen for testing the effect of fault injection.  \n",
        "\n",
        "### **Model details**\n",
        "**Task**: The task here is 'Text Predictiction'. \\\n",
        "**Input to the model**: Sequence of words as a string. Example: \"The chair is white and the table is\"\\\n",
        "**Model Prediction**: black\n",
        "\n",
        "The three ONNX models available and the reason for choosing gpt2-lm-head is given below:\n",
        "\n",
        "1. **GPT2** : It outputs only the **last hidden state**. Since it does not output any prediction score, the text prediction task cannot be performed\n",
        "  - Output of the model: (last_hidden_state, past)\n",
        "2. **GPT2-LM-HEAD**:  Outputs prediction scores of the language modeling head (scores for each vocabulary token before SoftMax). Therefore in the post-processing steps, softmax needs to be applied before converting the tokens to string.\n",
        "  - Output of the model: (prediction_scores, past)\n",
        "3. **GPT2-bs**: This GPT-2 model with generation can produce the result without any extra code or algorithm. It already embedded a beam search algorithm into the ONNX model, so there is NO Post-Processing code to inference. But, there is an error while converting this onnx model to LLVM IR. (https://github.com/DependableSystemsLab/LLTFI/issues/42) \n",
        "  - Output of the model: (Output tokens)\n",
        "\n",
        "Because of the above reason, gpt2-lm-head was chosen for testing fault injection.\n",
        "\n",
        "**References:** \\\n",
        "https://github.com/onnx/models/tree/main/text/machine_comprehension/gpt-2\n",
        "https://github.com/onnx/models/tree/main/text/machine_comprehension/gpt2-bs\n"
      ],
      "metadata": {
        "id": "hrmQP41wyQh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the working code for **gpt-bs**"
      ],
      "metadata": {
        "id": "89RwAmA0aFvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the gpt2-lm-head-bs-12 ONNX model\n",
        "!wget https://github.com/onnx/models/raw/main/text/machine_comprehension/gpt2-bs/model/gpt2-lm-head-bs-12.onnx"
      ],
      "metadata": {
        "id": "5EUCQxkxyPId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for executing the gpt2-lm-head-bs-12.onnx model and perform Text prediction task\n",
        "from transformers import GPT2Tokenizer\n",
        "from onnxruntime_extensions import PyOrtFunction\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "gpt2_all = PyOrtFunction.from_model('gpt2-lm-head-bs-12.onnx')\n",
        "encdict = tokenizer('Today is a beautiful sunny day. This is', padding=True, return_tensors='np')\n",
        "\n",
        "len_str = 30 # Length of the text you want the model to predict\n",
        "outputs = gpt2_all(encdict['input_ids'], encdict['attention_mask'].astype('float32'),30)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "wu2TtmqtyxeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output of the above cell (Predicted text in bold): \n",
        "\n",
        "Today is a beautiful sunny day. This is **the first time in my life that I have been able to see the sun. It is a beautiful day. This is the first time in my life**"
      ],
      "metadata": {
        "id": "F46BRGFi0Chq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2-lm-head"
      ],
      "metadata": {
        "id": "r5hh_JukzTYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text inputs to input.pb files\n",
        "\n",
        "input = []\n",
        "input.append(\"This chair is white and the table is\")\n",
        "input.append(\"It is bright and\")\n",
        "input.append(\"I am a doctor and I work at a\")\n",
        "input.append(\"I like playing with my\")\n",
        "input.append(\"A rose by any other name would smell as\")\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "for index in range(len(input)):\n",
        "  tokens = np.array(tokenizer.encode(input[index])) # Shape : (len,)\n",
        "  input_arr = tokens.reshape(1,1,-1) # Shape : (1,1,len)\n",
        "  input_tensor = numpy_helper.from_array(input_arr)\n",
        "  with open(\"input_{}.pb\".format(index), 'wb') as file:\n",
        "      file.write(input_tensor.SerializeToString())"
      ],
      "metadata": {
        "id": "ajhnfnQxyIf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute LLTFI with this input. Below is the code to convert LLTFI output to text"
      ],
      "metadata": {
        "id": "3wBQPMl012MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ROOT = os.getcwd()\n",
        "# Reads all the text files from 'prog_output' directory\n",
        "PROG_OUT = os.path.join(ROOT, 'prog_output')\n",
        "txtfiles = []\n",
        "\n",
        "for file in glob.glob(os.path.join(PROG_OUT, \"*.txt\")):\n",
        "    txtfiles.append(file)\n",
        "\n",
        "# List to store all the outputs\n",
        "listResArr = []\n",
        "for filename in txtfiles:\n",
        "  resforSingleInput = []\n",
        "  with open(filename, \"r\") as read_file:\n",
        "      resultJson = json.load(read_file)\n",
        "\n",
        "      for key, value in resultJson.items():\n",
        "          resforSingleInput.append(value['Data'])\n",
        "      listResArr.append(resforSingleInput)\n",
        "\n",
        "list_output_np = []\n",
        "# Reshape the output\n",
        "for elem in listResArr:\n",
        "  output_np = np.asarray(elem[0])\n",
        "  output_np = output_np.reshape(1,1,-1,50257)\n",
        "  list_output_np.append(output_np)"
      ],
      "metadata": {
        "id": "USKffUcO11SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For one output(one layeroutput.txt file) :"
      ],
      "metadata": {
        "id": "eXu-gc1n3K72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_np = list_output_np[0]"
      ],
      "metadata": {
        "id": "omaOG0Sj3KPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script to convert numpy output to text\n",
        "input_to_model = torch.tensor(\n",
        "      [[tokenizer.encode(\"It is bright and\", add_special_tokens=True)]]) # [1, 1, len]\n",
        "\n",
        "prev = input_to_model # [1, 1, len] Set prev as input in the first step\n",
        "prev = prev[0] # [1, len]\n",
        "output = prev\n",
        "\n",
        "logits = output_np[0]\n",
        "logits = logits[:, -1, :]\n",
        "logits = torch.tensor(logits)\n",
        "log_probs = F.softmax(logits, dim=-1)\n",
        "_, prev = torch.topk(log_probs, k=1, dim=-1)\n",
        "output2 = torch.cat((output, prev), dim=1)\n",
        "output = output2\n",
        "input_to_model = output2\n",
        "\n",
        "output1 = output2[:, len(tokens):].tolist()\n",
        "generated = 0\n",
        "batch_size = 1\n",
        "for i in range(batch_size):\n",
        "    generated += 1\n",
        "    text = tokenizer.decode(output1[i])\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "El82v7C12qXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below is the code to run inference in this jupyter notebook using the ONNX model directly**"
      ],
      "metadata": {
        "id": "1O8liLxV3dZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/onnx/models/raw/main/text/machine_comprehension/gpt-2/model/gpt2-lm-head-10.onnx"
      ],
      "metadata": {
        "id": "nfotGmtM4XAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(inputs):\n",
        "    return [[flatten(i) for i in inputs] if isinstance(inputs, (list, tuple)) else inputs]\n",
        "\n",
        "\n",
        "def update_flatten_list(inputs, res_list):\n",
        "    for i in inputs:\n",
        "        res_list.append(i) if not isinstance(i, (list, tuple)) else update_flatten_list(i, res_list)\n",
        "    return res_list\n",
        "\n",
        "def to_numpy(x):\n",
        "    if type(x) is not np.ndarray:\n",
        "        x = x.detach().cpu().numpy() if x.requires_grad else x.cpu().numpy()\n",
        "    return x\n",
        "\n",
        "def inference(inputs):\n",
        "    inputs_flatten = flatten(inputs)\n",
        "    inputs_flatten = update_flatten_list(inputs_flatten, [])\n",
        "    #outputs_flatten = flatten(outputs)\n",
        "    #outputs_flatten = update_flatten_list(outputs_flatten, [])\n",
        "\n",
        "    # Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n",
        "    # other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n",
        "    # based on the build flags) when instantiating InferenceSession.\n",
        "    # For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n",
        "    # onnxruntime.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\n",
        "    #sess = onnxruntime.InferenceSession(file)\n",
        "    ort_session = onnxruntime.InferenceSession(\"gpt2-lm-head-10.onnx\")\n",
        "    ort_inputs = dict((ort_session.get_inputs()[i].name, to_numpy(input)) for i, input in enumerate(inputs_flatten))\n",
        "    res = ort_session.run(None, ort_inputs)\n",
        "    return res"
      ],
      "metadata": {
        "id": "AwpKHMvG3auA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Model, GPT2LMHeadModel, GPT2Tokenizer\n",
        "import tensorflow as tf \n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "text = \"It is bright and\"\n",
        "\n",
        "input_to_model = torch.tensor(\n",
        "      [[tokenizer.encode(text, add_special_tokens=True)]]) # [1, 1, len]\n",
        "\n",
        "tokens = np.array(tokenizer.encode(text))\n",
        "\n",
        "prev = input_to_model # [1, 1, len]\n",
        "prev = prev[0] # [1, len]\n",
        "output = prev\n",
        "print(output)\n",
        "\n",
        "length = 10 # Length of the text you want the model to predict\n",
        "\n",
        "for i in range(length): \n",
        "  if(len(input_to_model.shape) == 2):\n",
        "    first = input_to_model.shape[0]\n",
        "    second = input_to_model.shape[1]\n",
        "    input_to_model = input_to_model.reshape(1,first,second)\n",
        "  result = inference(input_to_model)\n",
        "  print(result[0].shape)\n",
        "  logits = result[0][0]\n",
        "  logits = logits[:, -1, :]\n",
        "  logits = torch.tensor(logits)\n",
        "  log_probs = F.softmax(logits, dim=-1)\n",
        "  _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
        "  output2 = torch.cat((output, prev), dim=1)\n",
        "  output = output2\n",
        "  input_to_model = output2\n",
        "\n",
        "output1 = output2[:, len(tokens):].numpy().tolist()\n",
        "generated = 0\n",
        "batch_size = 1\n",
        "for i in range(batch_size):\n",
        "    generated += 1\n",
        "    text = tokenizer.decode(output1[i])\n",
        "    print(text)\n",
        "\n",
        "## Note: The above code uses pytorch. "
      ],
      "metadata": {
        "id": "yOi3f2lJ37iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: sunny in the morning, and the sun is shining"
      ],
      "metadata": {
        "id": "lvyUBC12dLmH"
      }
    }
  ]
}